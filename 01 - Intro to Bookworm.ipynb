{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Bookworm\n",
    "## Motivation\n",
    "_Infinite Jest_ is a very long and complicated novel. There are a lot of brilliant resources connected to the book, which aim to help the reader stay afloat amongst the chaos of David Foster Wallace's obscure language, interwoven timelines and narratives, and the sprawling networks of characters. The [Infinite Jest Wiki](http://infinitejest.wallacewiki.com/david-foster-wallace/index.php?title=Infinite_Jest_Page_by_Page), for example, is insanely well documented and I'd recommend it to anyone reading the book.  \n",
    "One of the most interesting resources I found while reading was Sam Potts' [Infinite Jest Diagram](http://www.sampottsinc.com/ij/).  \n",
    "\n",
    "![alt text](https://a.fastcompany.net/upload/IJ_Diagram-Huge-A.jpg \"IJmap\")\n",
    "\n",
    "I genuinely went back to the image once or twice to work out who a character was and how they were connected to the scene. It's a fun thing to have access to while reading something so deliberately scattered.  \n",
    "However, Infinite Jest isn't the only \"big\" book I've ever read, and as far as I know the network above was drawn up entirely by hand. I thought it would be nice to have something like this for anything I was reading. It might also function as an interesting learning resource - either for kids at a young, early-reader stage with simple books and small character networks, or for people learning about network analysis who have never bothered reading [Les Miserables](https://bl.ocks.org/mbostock/4062045) (again, as far as I know the standard Les Mis dataset was put together entirely by hand).  \n",
    "I thought that with a bit of thought and testing, this process was probably automatable. It is. I can now feed bookworm any novel and have it churn out a pretty network like the one above in seconds, without me having to know anything about the book. Also, by virtue of the way character connections are measured, it can tell you the relative strength of all links between characters.\n",
    "\n",
    "## Getting Started\n",
    "Go and have a quick look at the stripped down bookworm code in [bookworm.py](./bookworm.py). The first thing we're going to do is load in all of those functions. I'll also do some explanation of how each function works along the way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bookworm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next thing to do is load in the book and the characters. These operations are both pretty simple. The book is loaded in as one long string from a `.txt` file. Character lists are stored in a `.csv`, with all potential names for a character stored on each row. They're loaded in as tuples of names in a list of characters.  \n",
    "\n",
    "Then we split the book down into sections. Bookworm works by looking for _coocurrence_ of characters in these sections of the text as a proxy for their connectedness. It's a very simple trick which works stupidly well.  \n",
    "There are a few ways we can break down the book into sections:\n",
    "- `get_sentence_sequences()` uses [NLTK](http://www.nltk.org/)'s standard `.tokenize()` function to split the book into sentences.  \n",
    "- `get_word_sequences()` uses [NLTK](http://www.nltk.org/)'s `word_tokenize()` to split the book into words, of which it will then select ordered lists of length `n` (default 40).  \n",
    "- `get_character_sequences()` uses python builtins to split it into substrings of length `n` (default 200).  \n",
    "\n",
    "Fundamentally, they all return a list of strings which each cover a very small section of the novel. For simplicity's sake we're going to use the sentence-wise splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = load_book('data/raw/ij.txt', lower=True)\n",
    "characters = load_characters('data/raw/characters_ij.csv')\n",
    "\n",
    "sequences = get_sentence_sequences(book)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now comes the fun bit. We've assembled our cast, and moved the text that they inhabit into a nice, machine-interpretable format.  \n",
    "What we want to generate now is the blank table below which describes the presence of a character in a sentence. At this point, Bookworm hasn't really 'read' any of the text so all of the interactions between characters and sentences (where each cell in the table represents an interaction) are set to 0:\n",
    "\n",
    "|            | character 1 | character 2 | character 3 |\n",
    "|------------|-------------|-------------|-------------|\n",
    "| sentence 1 | 0           | 0           | 0           |\n",
    "| sentence 2 | 0           | 0           | 0           |\n",
    "| sentence 3 | 0           | 0           | 0           |\n",
    "| sentence 4 | 0           | 0           | 0           |\n",
    "\n",
    "\n",
    "However, tuples of names aren't that nicely placed into tables as column headings, and entire sentences (especially those in Infinite Jest) are inappropriately long to be used as row indexes. To get around this aesthetic lumpiness, we can set up a hash table for each of the lists, allowing us to move backwards and forwards quickly and easily between the tuple/text themselves and their numeric fingerprints.  \n",
    "Note: we'll be using pandas to build the table above so that hashing is obviously already being done automatically under the hood - doing it explicitly is a purely aesthetic choice.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hash_to_sequence, sequence_to_hash = get_hashes(sequences)\n",
    "hash_to_character, character_to_hash = get_hashes(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we've got:\n",
    "\n",
    "|                  | hash(character 1) | hash(character 2) | hash(character 3) |\n",
    "|------------------|-------------------|-------------------|-------------------|\n",
    "| hash(sentence 1) | 0                 | 0                 | 0                 |\n",
    "| hash(sentence 2) | 0                 | 0                 | 0                 |\n",
    "| hash(sentence 3) | 0                 | 0                 | 0                 |\n",
    "| hash(sentence 4) | 0                 | 0                 | 0                 |\n",
    "\n",
    "| hash              | character   |\n",
    "|-------------------|-------------|\n",
    "| hash(character 1) | character 1 |\n",
    "| hash(character 2) | character 2 |\n",
    "| hash(character 3) | character 3 |\n",
    "\n",
    "| hash             | sentence   |\n",
    "|------------------|------------|\n",
    "| hash(sentence 1) | sentence 1 |\n",
    "| hash(sentence 2) | sentence 2 |\n",
    "| hash(sentence 3) | sentence 3 |\n",
    "| hash(sentence 4) | sentence 4 |\n",
    "\n",
    "We also have the hash tables in reverse too, just in case they become useful later on.\n",
    "\n",
    "The first bit of the `find_connections()` function sets up the blank table above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = find_connections(sequences, characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it iterates through the list of sentences it has been fed, checking for an instance of each character. If it finds a character in the sentence, it marks their presence with a 1.  \n",
    "So if **character 1** appears with **character 2** in sentence 1, and with **character 3** in sentence 2, we would see the following, with the rest of the cells remaining blank:\n",
    "\n",
    "|                  | hash(character 1) | hash(character 2) | hash(character 3) |\n",
    "|------------------|-------------------|-------------------|-------------------|\n",
    "| hash(sentence 1) | 1                 | 1                 | 0                 |\n",
    "| hash(sentence 2) | 1                 | 0                 | 1                 |\n",
    "| hash(sentence 3) | 0                 | 0                 | 0                 |\n",
    "| hash(sentence 4) | 0                 | 0                 | 0                 |\n",
    "\n",
    "The next stage is the enumeration of coocurence. We can compute this very quickly with the `numpy` dot product of the table with its own transpose. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurence = calculate_cooccurence(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`calculate_cooccurence()` does this computation and then wipes out any interaction of a character with themselves. For the table above, this would give us:\n",
    "\n",
    "|                   | hash(character 1) | hash(character 2) | hash(character 3) |\n",
    "|-------------------|-------------------|-------------------|-------------------|\n",
    "| hash(character 1) | 0                 | 1                 | 1                 |\n",
    "| hash(character 2) | 1                 | 0                 | 0                 |\n",
    "| hash(character 3) | 1                 | 0                 | 0                 |\n",
    "\n",
    "showing that **character 1** has interacted with **character 2** and **character 3**, but **character 2** and **character 3** haven't interacted. Note the symmetry across the diagonal...\n",
    "\n",
    "Of course, the example table here is miniscule in comparison to the dozens of characters who might turn up in a reasonably sized novel, and the thousands of opportunities they have to interact. The coocurence matrix in reality is likely to contain much larger numbers between characters who regularly appear in the same sentences. Unless we're working with a _really_ tiny, incestuous network, this coocurence matrix is also probably going to be pretty sparse. For that reason it'll often make sense to store it as a sparse matrix..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cooccurence = cooccurence.to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully you now have a decent understanding of what Bookworm does, and how.\n",
    "\n",
    "We can now show some results! Despite describing a set of tiny matrices above, we've really been computing all of Infinite Jest's massiveness while working through the notebook.\n",
    "\n",
    "We can print the strongest relationships for a chosen character using the function below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def print_five_closest(character):\n",
    "    print('-'*len(str(character))\n",
    "          + '\\n' + str(character) + '\\n'\n",
    "          + '-'*len(str(character)))\n",
    "    \n",
    "    top_five = (cooccurence[hash(character)]\n",
    "                .sort_values(ascending=False)\n",
    "                .index.values\n",
    "                [:5])\n",
    "    \n",
    "    for hashed_name in top_five:\n",
    "        print(hash_to_character[hashed_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying this to 5 characters at random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "('hester ', 'thrale ')\n",
      "----------------------\n",
      "('gately ', 'don ')\n",
      "('randy ', 'lenz ')\n",
      "('himself ', 'mad stork ', 'jim icandenza ', 'james incandenza ')\n",
      "('bruce green ',)\n",
      "('lateral ', 'alice moore ')\n",
      "\n",
      "-------------------------------------\n",
      "('fdv ', 'harde ', 'fall down very ')\n",
      "-------------------------------------\n",
      "('delint ',)\n",
      "('lateral ', 'alice moore ')\n",
      "('kenkle ',)\n",
      "('schtitt ',)\n",
      "('quo vadis ',)\n",
      "\n",
      "-------------------------------------\n",
      "('the moms ', 'avril ', 'mondragon ')\n",
      "-------------------------------------\n",
      "('hal ',)\n",
      "('orin ',)\n",
      "('mario ',)\n",
      "('himself ', 'mad stork ', 'jim icandenza ', 'james incandenza ')\n",
      "('joelle ', 'van dyne ', 'lucille ')\n",
      "\n",
      "-------------------------------------\n",
      "('the moms ', 'avril ', 'mondragon ')\n",
      "-------------------------------------\n",
      "('hal ',)\n",
      "('orin ',)\n",
      "('mario ',)\n",
      "('himself ', 'mad stork ', 'jim icandenza ', 'james incandenza ')\n",
      "('joelle ', 'van dyne ', 'lucille ')\n",
      "\n",
      "------------------------\n",
      "('bridget tenderhole ',)\n",
      "------------------------\n",
      "('stokeley \"dark star\" mcnair ', 'stokeley ', 'dark star ', 'darkstar ', 'mcnair ')\n",
      "('poor tony ', 'krause ')\n",
      "('bertraund ',)\n",
      "('susan t. cheese ',)\n",
      "('quo vadis ',)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from random import randint\n",
    "\n",
    "for i in range(5):\n",
    "    print_five_closest(characters[randint(0, len(characters))])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those all seem to make sense... Lets try with a few characters who we know about in more detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "('the moms ', 'avril ', 'mondragon ')\n",
      "-------------------------------------\n",
      "('hal ',)\n",
      "('orin ',)\n",
      "('mario ',)\n",
      "('himself ', 'mad stork ', 'jim icandenza ', 'james incandenza ')\n",
      "('joelle ', 'van dyne ', 'lucille ')\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('the moms ', 'avril ', 'mondragon '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "('joelle ', 'van dyne ', 'lucille ')\n",
      "------------------------------------\n",
      "('orin ',)\n",
      "('gately ', 'don ')\n",
      "('the moms ', 'avril ', 'mondragon ')\n",
      "('erdedy ',)\n",
      "('himself ', 'mad stork ', 'jim icandenza ', 'james incandenza ')\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('joelle ', 'van dyne ', 'lucille '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------\n",
      "('pemulis ',)\n",
      "-------------\n",
      "('hal ',)\n",
      "('trevor \"axhandle\" axford ', 'axford ', 'axhandle ')\n",
      "('jim troeltsch ', 'troeltsch ')\n",
      "('james struck ', 'struck ')\n",
      "('ted schacht ', 'schacht ')\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('pemulis ',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------\n",
      "('bruce green ',)\n",
      "-----------------\n",
      "('randy ', 'lenz ')\n",
      "('gately ', 'don ')\n",
      "('himself ', 'mad stork ', 'jim icandenza ', 'james incandenza ')\n",
      "('kate gompert ', 'gompert ')\n",
      "('burt f. smith ', 'burt ')\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('bruce green ',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep...  Go and look at the diagram in the README and compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Same code, different book\n",
    "Lets run the whole thing for an entirely different book and see whether we get similarly positive results. This time, Harry Potter and The Philosopher's Stone - chosen because you're more likely to have some contextual knowledge of who's who and what's what in that book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "book = load_book('data/raw/hp_philosophers_stone.txt', lower=True)\n",
    "characters = load_characters('data/raw/characters_hp.csv')\n",
    "sequences = get_sentence_sequences(book)\n",
    "\n",
    "hash_to_sequence, sequence_to_hash = get_hashes(sequences)\n",
    "hash_to_character, character_to_hash = get_hashes(characters)\n",
    "\n",
    "df = find_connections(sequences, characters)\n",
    "cooccurence = calculate_cooccurence(df).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('vernon ', ' dursley '),\n",
       " ('petunia ', ' dursley '),\n",
       " ('dudley ', ' duddy '),\n",
       " ('lily ',),\n",
       " ('james ',)]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "characters[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "('harry ', ' potter ')\n",
      "----------------------\n",
      "('ron ', ' weasley ')\n",
      "('hermione ', ' granger ')\n",
      "('hagrid ', ' rubeus ')\n",
      "('snape ', ' severus ')\n",
      "('dudley ', ' duddy ')\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('harry ', ' potter '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "('voldemort ', ' lord ', ' you-know-who ')\n",
      "------------------------------------------\n",
      "('harry ', ' potter ')\n",
      "('quirrell ',)\n",
      "('snape ', ' severus ')\n",
      "('dumbledore ', ' albus ')\n",
      "('ron ', ' weasley ')\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('voldemort ', ' lord ', ' you-know-who '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------\n",
      "('crabbe ',)\n",
      "------------\n",
      "('goyle ',)\n",
      "('draco ', ' malfoy ')\n",
      "('harry ', ' potter ')\n",
      "('neville ', ' longbottom ')\n",
      "('fred ',)\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('crabbe ',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "('fred ',)\n",
      "----------\n",
      "('george ',)\n",
      "('ron ', ' weasley ')\n",
      "('harry ', ' potter ')\n",
      "('sorting ', ' hat ')\n",
      "('crabbe ',)\n"
     ]
    }
   ],
   "source": [
    "print_five_closest(('fred ',))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully that's enough proof that this thing works well. Go and look at the [next notebook](./02%20-%20Visualising Networks.ipynb) which covers a bit more network analysis and some ways in which we can visualise the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
